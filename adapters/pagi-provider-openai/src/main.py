import asyncio
import json
import logging
import os

import grpc

from .config import load_config


log = logging.getLogger("pagi.provider.openai")


def _import_contracts():
    # Generated by ./tools/generate-protos.sh
    from pagi_contracts import agent_pb2, agent_pb2_grpc  # type: ignore

    return agent_pb2, agent_pb2_grpc


def _message_role_to_openai(role: int) -> str:
    # From contracts/agent.proto MessageRole.
    return {
        1: "system",
        2: "user",
        3: "assistant",
        4: "tool",
    }.get(int(role), "user")


def _to_openai_messages(messages):
    out = []
    for m in messages:
        parts = []
        for p in m.content:
            kind = p.WhichOneof("part")
            if kind == "text":
                parts.append({"type": "text", "text": p.text.text})
            elif kind == "image":
                parts.append({"type": "image_url", "image_url": {"url": p.image.url}})
            elif kind == "audio":
                parts.append({"type": "text", "text": f"[audio] {p.audio.url}"})
            elif kind == "file":
                parts.append({"type": "text", "text": f"[file {p.file.mime_type}] {p.file.url}"})

        if not parts:
            parts = [{"type": "text", "text": ""}]

        out.append({"role": _message_role_to_openai(m.role), "content": parts})
    return out


def _to_openai_tools(tools):
    out = []
    for t in tools:
        schema = {}
        if getattr(t, "parameters_json_schema", ""):
            try:
                schema = json.loads(t.parameters_json_schema)
            except Exception:
                schema = {}
        out.append(
            {
                "type": "function",
                "function": {
                    "name": t.name,
                    "description": t.description or "",
                    "parameters": schema,
                },
            }
        )
    return out


class ProviderService:
    def __init__(self, adapter_id: str, default_model: str):
        self.adapter_id = adapter_id
        self.default_model = default_model

    async def Process(self, request, context):  # noqa: N802
        agent_pb2, _ = _import_contracts()

        # Resolve model: metadata.routed_model > preferred_model > default
        routed_model = None
        try:
            routed_model = request.metadata.get("routed_model")
        except Exception:
            routed_model = None

        model = routed_model or (request.preferred_model or None) or self.default_model

        # If no API key, return a stubbed response (keeps bare-metal demo working).
        if not os.getenv("OPENAI_API_KEY"):
            payload = {
                "provider": "openai",
                "model": model,
                "note": "OPENAI_API_KEY not set; returning stub response",
                "received_messages": len(request.messages),
                "received_tools": len(request.tools),
            }
            return agent_pb2.CanonicalAIResponse(
                request_id=request.request_id,
                adapter_id=self.adapter_id,
                json=json.dumps(payload),
            )

        from openai import OpenAI

        client = OpenAI()
        messages = _to_openai_messages(request.messages)
        tools = _to_openai_tools(request.tools)

        kwargs = {
            "model": model,
            "messages": messages,
        }

        if tools:
            kwargs["tools"] = tools

        # Constraints are proto scalars; treat 0 as unset.
        if request.constraints.max_tokens:
            kwargs["max_tokens"] = int(request.constraints.max_tokens)
        if request.constraints.temperature:
            kwargs["temperature"] = float(request.constraints.temperature)

        resp = client.chat.completions.create(**kwargs)
        text = ""
        try:
            text = resp.choices[0].message.content or ""
        except Exception:
            text = ""

        payload = {
            "provider": "openai",
            "model": model,
            "text": text,
        }

        return agent_pb2.CanonicalAIResponse(
            request_id=request.request_id,
            adapter_id=self.adapter_id,
            json=json.dumps(payload),
        )


async def register_with_core(cfg) -> None:
    agent_pb2, agent_pb2_grpc = _import_contracts()

    channel = grpc.aio.insecure_channel(cfg.core_grpc)
    stub = agent_pb2_grpc.AdapterRegistryStub(channel)

    req = agent_pb2.RegisterAdapterRequest(
        adapter_id=cfg.adapter_id,
        endpoint=f"http://{cfg.bind}",
        capabilities=agent_pb2.AdapterCapabilities(
            streaming=False,
            token_count=False,
            model_route=False,
            embed_cache=False,
        ),
        version=cfg.version,
    )

    resp = await stub.Register(req)
    if not resp.ok:
        raise RuntimeError("core rejected adapter registration")


async def serve() -> None:
    logging.basicConfig(level=logging.INFO)
    cfg = load_config()
    log.info("starting openai provider adapter id=%s bind=%s core_grpc=%s", cfg.adapter_id, cfg.bind, cfg.core_grpc)

    agent_pb2, agent_pb2_grpc = _import_contracts()

    server = grpc.aio.server()
    agent_pb2_grpc.add_AdapterServiceServicer_to_server(
        ProviderService(cfg.adapter_id, cfg.default_model),
        server,
    )
    server.add_insecure_port(cfg.bind)

    await server.start()
    await register_with_core(cfg)
    log.info("registered with core")

    await server.wait_for_termination()


if __name__ == "__main__":
    asyncio.run(serve())

